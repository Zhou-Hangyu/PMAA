{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5c461e-48cf-496e-9549-bd0ab461368a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, torch\n",
    "if \"ck696\" in os.getcwd():\n",
    "    sys.path.append(\"/share/hariharan/ck696/allclear\")\n",
    "else:\n",
    "    sys.path.append(\"/share/hariharan/cloud_removal/allclear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcf3a1-f415-493c-867d-a8536f126dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ck696/.conda/envs/H3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ck696/.conda/envs/H3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(root='/share/hariharan/cloud_removal', root_file='/share/hariharan/ck696/allclear/baselines/PMAA/data', cloud_model_path='./data/Feature_Extrator_FS2.pth', save_model_path='./checkpoints0515', dataset_name='CTGAN_Sen2_MTC', load_gen='', load_dis='', n_epochs=100, gan_mode='lsgan', optimizer='AdamW', lr=0.0005, workers=1, batch_size=1, lambda_L1=100.0, lambda_aux=50.0, in_channel=4, out_channel=4, image_size=256, aux_loss=False, label_noise=False, gpu_id='0', manual_seed=2022)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mck696\u001b[0m (\u001b[33mcornell-kao\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/hariharan/ck696/allclear/baselines/PMAA/wandb/run-20240522_015428-ghefgxtl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cornell-kao/allclear-pmaa-v1/runs/ghefgxtl' target=\"_blank\">PMAA_SEN2MTC_lm100_la50_bs1_seed2022</a></strong> to <a href='https://wandb.ai/cornell-kao/allclear-pmaa-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cornell-kao/allclear-pmaa-v1' target=\"_blank\">https://wandb.ai/cornell-kao/allclear-pmaa-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cornell-kao/allclear-pmaa-v1/runs/ghefgxtl' target=\"_blank\">https://wandb.ai/cornell-kao/allclear-pmaa-v1/runs/ghefgxtl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load cloud_detection_model\n",
      "Load ours model\n",
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[0/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0005000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[0/100]:   4% 99/2380 [00:44<17:05,  2.22 step/s, D_fake=0.0363, D_real=0.1192, G_GAN=0.7252, G_L1=19.9167, G_L1_total=2818.8071]\n",
      "Valid[0/100]: 100% 350/350 [00:27<00:00, 12.56 step/s, loss_val=68.9412, psnr=11.119, ssim=0.155]\n",
      "Train[1/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[1/100]:   4% 99/2380 [00:59<22:41,  1.68 step/s, D_fake=0.0038, D_real=0.0318, G_GAN=0.8873, G_L1=23.6234, G_L1_total=1849.7564]\n",
      "Valid[1/100]: 100% 350/350 [00:44<00:00,  7.89 step/s, loss_val=52.9647, psnr=12.264, ssim=0.251]\n",
      "Train[2/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[2/100]:   4% 99/2380 [01:03<24:32,  1.55 step/s, D_fake=0.0021, D_real=0.0106, G_GAN=0.9997, G_L1=15.5186, G_L1_total=1495.4900]\n",
      "Valid[2/100]: 100% 350/350 [00:22<00:00, 15.23 step/s, loss_val=45.5349, psnr=13.030, ssim=0.247]\n",
      "Train[3/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[3/100]:   4% 99/2380 [00:42<16:19,  2.33 step/s, D_fake=0.0022, D_real=0.0084, G_GAN=0.9831, G_L1=10.0068, G_L1_total=1349.5901]\n",
      "Valid[3/100]: 100% 350/350 [00:24<00:00, 14.33 step/s, loss_val=40.5623, psnr=13.780, ssim=0.257]\n",
      "Train[4/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[4/100]:   4% 99/2380 [00:48<18:32,  2.05 step/s, D_fake=0.0457, D_real=0.0994, G_GAN=0.7402, G_L1=12.5679, G_L1_total=1257.6988]\n",
      "Valid[4/100]: 100% 350/350 [00:24<00:00, 14.34 step/s, loss_val=40.3342, psnr=11.958, ssim=0.210]\n",
      "Train[5/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[5/100]:   4% 99/2380 [00:43<16:37,  2.29 step/s, D_fake=0.0038, D_real=0.0044, G_GAN=1.0299, G_L1=11.4967, G_L1_total=1093.0556]\n",
      "Valid[5/100]: 100% 350/350 [00:24<00:00, 14.33 step/s, loss_val=35.2642, psnr=12.554, ssim=0.202]\n",
      "Train[6/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[6/100]:   4% 99/2380 [00:42<16:10,  2.35 step/s, D_fake=0.7068, D_real=0.0534, G_GAN=0.2856, G_L1=7.7998, G_L1_total=1046.0294] \n",
      "Valid[6/100]: 100% 350/350 [00:24<00:00, 14.31 step/s, loss_val=29.5601, psnr=15.760, ssim=0.294]\n",
      "Train[7/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[7/100]:   4% 99/2380 [00:42<16:14,  2.34 step/s, D_fake=0.1196, D_real=0.0435, G_GAN=0.6889, G_L1=17.5806, G_L1_total=980.5232]\n",
      "Valid[7/100]: 100% 350/350 [00:26<00:00, 13.03 step/s, loss_val=27.5120, psnr=15.424, ssim=0.305]\n",
      "Train[8/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[8/100]:   4% 99/2380 [00:41<15:45,  2.41 step/s, D_fake=0.0354, D_real=0.0092, G_GAN=1.1264, G_L1=6.0892, G_L1_total=976.3579] \n",
      "Valid[8/100]: 100% 350/350 [00:32<00:00, 10.92 step/s, loss_val=26.1273, psnr=15.601, ssim=0.302]\n",
      "Train[9/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[9/100]:   4% 99/2380 [00:41<15:47,  2.41 step/s, D_fake=0.0056, D_real=0.0135, G_GAN=1.0666, G_L1=7.0359, G_L1_total=955.7789] \n",
      "Valid[9/100]: 100% 350/350 [00:30<00:00, 11.50 step/s, loss_val=26.4964, psnr=14.800, ssim=0.278]\n",
      "Train[10/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[10/100]:   4% 99/2380 [00:47<18:16,  2.08 step/s, D_fake=0.0021, D_real=0.0472, G_GAN=0.8571, G_L1=7.0678, G_L1_total=910.3208]   \n",
      "Valid[10/100]: 100% 350/350 [00:24<00:00, 14.33 step/s, loss_val=24.5014, psnr=15.910, ssim=0.310]\n",
      "Train[11/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[11/100]:   4% 99/2380 [00:41<16:04,  2.36 step/s, D_fake=0.0202, D_real=0.0568, G_GAN=0.7127, G_L1=10.1483, G_L1_total=850.4427]\n",
      "Valid[11/100]: 100% 350/350 [00:24<00:00, 14.34 step/s, loss_val=23.6677, psnr=16.037, ssim=0.311]\n",
      "Train[12/100]:   0% 0/2380 [00:00<?, ? step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning rate = 0.0004825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train[12/100]:   4% 99/2380 [00:41<16:07,  2.36 step/s, D_fake=0.0510, D_real=0.0696, G_GAN=0.7077, G_L1=12.2474, G_L1_total=760.3212]\n",
      "Valid[12/100]:  50% 176/350 [00:12<00:12, 14.34 step/s, loss_val=12.9524]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"5f04d2ce100707f23b71379f67f28901d496edda\"\n",
    "# os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "import warnings\n",
    "from utils import set_requires_grad, get_rgb, GANLoss\n",
    "from utils import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset_new import Sen2_MTC\n",
    "from model.fe import FeatureExtractor\n",
    "from model.pmaa import PMAA\n",
    "from model.discriminator import Discriminator\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\"\"\"Path\"\"\"\n",
    "# parser.add_argument(\"--root\", type=str, default='/share/hariharan/ck696',\n",
    "parser.add_argument(\"--root\", type=str, default= '/share/hariharan/cloud_removal', \n",
    "                    choices=['/scratch/allclear', '/share/hariharan/ck696'],\n",
    "                    help=\"Path to dataset\")\n",
    "parser.add_argument(\"--root_file\", type=str, default='/share/hariharan/ck696/allclear/baselines/PMAA/data', \n",
    "                    help=\"Path to dataset\")\n",
    "parser.add_argument(\"--cloud_model_path\", type=str,\n",
    "                    default='./data/Feature_Extrator_FS2.pth', help=\"path to feature extractor model\")\n",
    "parser.add_argument(\"--save_model_path\", type=str,\n",
    "                    default='./checkpoints0515', help=\"Path to save model\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, choices=[\"CTGAN_Sen2_MTC\", \"AllClear\"],\n",
    "                    default='CTGAN_Sen2_MTC', help=\"name of the dataset\")\n",
    "parser.add_argument(\"--load_gen\", type=str, default='',\n",
    "                    help=\"path to the model of generator\")\n",
    "parser.add_argument(\"--load_dis\", type=str, default='',\n",
    "                    help=\"path to the model of discriminator\")\n",
    "\n",
    "\"\"\"Parameters\"\"\"\n",
    "parser.add_argument(\"--n_epochs\", type=int,\n",
    "                    default=100, help=\"Number of epochs\")\n",
    "parser.add_argument(\"--gan_mode\", type=str, default='lsgan',\n",
    "                    help=\"Which gan mode(lsgan/vanilla)\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default='AdamW',\n",
    "                    help=\"optimizer you want to use(AdamW/SGD)\")\n",
    "parser.add_argument(\"--lr\", type=float, default=5e-4, help=\"learning rate\")\n",
    "parser.add_argument(\"--workers\", type=int, default=1,\n",
    "                    help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--batch_size\", type=int,\n",
    "                    default=1, help=\"size of the batches\")\n",
    "parser.add_argument('--lambda_L1', type=float,\n",
    "                    default=100.0, help='weight for L1 loss')\n",
    "parser.add_argument('--lambda_aux', type=float,\n",
    "                    default=50.0, help='weight for aux loss')\n",
    "parser.add_argument(\"--in_channel\", type=int, default=4,\n",
    "                    help=\"the number of input channels\")\n",
    "parser.add_argument(\"--out_channel\", type=int, default=4,\n",
    "                    help=\"the number of output channels\")\n",
    "parser.add_argument(\"--image_size\", type=int,\n",
    "                    default=256, help=\"crop size\")\n",
    "parser.add_argument(\"--aux_loss\", action='store_true',\n",
    "                    help=\"whether use auxiliary loss(1/0)\")\n",
    "parser.add_argument(\"--label_noise\", action='store_true',\n",
    "                    help=\"whether to add noise on the label of gan training\")\n",
    "\n",
    "\"\"\"base_options\"\"\"\n",
    "parser.add_argument(\"--gpu_id\", type=str, default='0', help=\"gpu id\")\n",
    "parser.add_argument(\"--manual_seed\", type=int,\n",
    "                    default=2022, help=\"random_seed you want\")\n",
    "\n",
    "opt, _ = parser.parse_known_args()\n",
    "print(opt)\n",
    "\n",
    "if opt.dataset_name == \"CTGAN_Sen2_MTC\": dataset_name_ = \"SEN2MTC\"\n",
    "if opt.dataset_name == \"AllClear\": dataset_name_ = \"AllClear\"\n",
    "run_name = f\"PMAA_{dataset_name_}_lm{int(opt.lambda_L1)}_la{int(opt.lambda_aux)}_bs{opt.batch_size}_seed{opt.manual_seed}\"\n",
    "wandb.init(project=\"allclear-pmaa-v1\", name=run_name, config=opt)\n",
    "\n",
    "os.makedirs(os.path.join(opt.save_model_path,\n",
    "            run_name), exist_ok=True)\n",
    "fixed_seed(opt.manual_seed)\n",
    "\n",
    "if opt.dataset_name == \"AllClear\":\n",
    "\n",
    "    from dataset.dataloader_v1 import CRDataset\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    import json\n",
    "    with open('/scratch/allclear/metadata/v3/test_tx3_v1.json') as f:\n",
    "        metadata = json.load(f)\n",
    "    #len=3168\n",
    "    train_data = CRDataset(metadata, \n",
    "                        selected_rois=\"all\", \n",
    "                        main_sensor=\"s2_toa\", \n",
    "                        aux_sensors=[],\n",
    "                        aux_data=[],\n",
    "                        format=\"stp\",\n",
    "                        target=\"s2p\",\n",
    "                        tx=3)\n",
    "    train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True,\n",
    "                              num_workers=opt.workers, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "    \n",
    "elif opt.dataset_name == \"CTGAN_Sen2_MTC\":\n",
    "    # len=1190\n",
    "    train_data = Sen2_MTC(opt, 'train')\n",
    "    train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True,\n",
    "                              num_workers=opt.workers, drop_last=True, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "val_data = Sen2_MTC(opt, mode='val')\n",
    "val_loader = DataLoader(val_data, batch_size=opt.batch_size, shuffle=False,\n",
    "                        num_workers=opt.workers, drop_last=False, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# assert 0 == 1\n",
    "print('Load cloud_detection_model')\n",
    "cloud_detection_model = FeatureExtractor()\n",
    "cloud_detection_model.load_state_dict(torch.load(opt.cloud_model_path))\n",
    "cloud_detection_model.eval()\n",
    "set_requires_grad(cloud_detection_model, False)\n",
    "\n",
    "print('Load ours model')\n",
    "GEN = PMAA(32, 4)\n",
    "\n",
    "def replace_batchnorm(model):\n",
    "    for name, child in model.named_children():\n",
    "        if isinstance(child, torch.nn.BatchNorm2d):\n",
    "            child: torch.nn.BatchNorm2d = child\n",
    "            setattr(model, name, torch.nn.InstanceNorm2d(child.num_features))\n",
    "        else:\n",
    "            replace_batchnorm(child)\n",
    "replace_batchnorm(GEN)\n",
    "DIS = Discriminator()\n",
    "\n",
    "if opt.load_gen and opt.load_dis:\n",
    "    print('loading pre-trained model')\n",
    "    GEN.load_state_dict(torch.load(opt.load_gen))\n",
    "    DIS.load_state_dict(torch.load(opt.load_dis))\n",
    "\n",
    "if opt.optimizer == 'AdamW':\n",
    "    optimizer_G = torch.optim.AdamW(\n",
    "        GEN.parameters(), lr=opt.lr, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "    optimizer_D = torch.optim.AdamW(\n",
    "        DIS.parameters(), lr=opt.lr, betas=(0.5, 0.999), weight_decay=5e-4)\n",
    "if opt.optimizer == 'SGD':\n",
    "    optimizer_G = torch.optim.SGD(\n",
    "        GEN.parameters(), lr=opt.lr, momentum=0.9, nesterov=True)\n",
    "    optimizer_D = torch.optim.SGD(\n",
    "        DIS.parameters(), lr=opt.lr, momentum=0.9, nesterov=True)\n",
    "\n",
    "# def train(opt, model_GEN, model_DIS, cloud_detection_model, optimizer_G, optimizer_D, train_loader, val_loader):\n",
    "\n",
    "model_GEN = GEN\n",
    "model_DIS = DIS\n",
    "\n",
    "def preprocess_images(opt, batch, pmaa_bands=(3,2,1,7)):\n",
    "    if opt.dataset_name == \"CTGAN_Sen2_MTC\":\n",
    "        real_A, real_B, _ = batch\n",
    "        return real_A[0].cuda(), real_A[1].cuda(), real_A[2].cuda(), real_B.cuda()\n",
    "    elif opt.dataset_name == \"AllClear\":        \n",
    "        real_As = batch[\"input_images\"] * 2 - 1\n",
    "        real_Bs = batch[\"target\"] * 2 - 1\n",
    "        return real_As[:,pmaa_bands,0].cuda(), real_As[:,pmaa_bands,1].cuda(), real_As[:,pmaa_bands,2].cuda(), real_Bs[:,pmaa_bands,0].cuda()\n",
    "\n",
    "def train(opt, model_GEN, model_DIS, cloud_detection_model, optimizer_G, optimizer_D, train_loader, val_loader):\n",
    "    writer = SummaryWriter('runs29/%s' % opt.dataset_name)\n",
    "\n",
    "    noise = opt.label_noise\n",
    "    criterionGAN = GANLoss(opt.gan_mode)\n",
    "    criterionL1 = torch.nn.L1Loss()\n",
    "    criterionMSE = nn.MSELoss()\n",
    "\n",
    "    if cuda:\n",
    "        criterionGAN = criterionGAN.cuda()\n",
    "        criterionL1 = criterionL1.cuda()\n",
    "        criterionMSE = criterionMSE.cuda()\n",
    "        cloud_detection_model = cloud_detection_model.cuda()\n",
    "        model_GEN = model_GEN.cuda()\n",
    "        model_DIS = model_DIS.cuda()\n",
    "\n",
    "    \"\"\"lr_scheduler\"\"\"\n",
    "    scheduler_G = CosineAnnealingLR(\n",
    "        optimizer_G, T_max=opt.n_epochs, eta_min=1e-6)\n",
    "    scheduler_D = CosineAnnealingLR(\n",
    "        optimizer_D, T_max=opt.n_epochs, eta_min=1e-6)\n",
    "\n",
    "    \"\"\"training\"\"\"\n",
    "    train_update = 0\n",
    "    psnr_max = 0.\n",
    "    ssim_max = 0.\n",
    "\n",
    "    print('Start training!')\n",
    "    for epoch in range(opt.n_epochs):\n",
    "        model_GEN.train()\n",
    "        model_DIS.train()\n",
    "\n",
    "        pbar = tqdm.tqdm(total=len(train_loader), ncols=0,\n",
    "                         desc=\"Train[%d/%d]\" % (epoch, opt.n_epochs), unit=\" step\")\n",
    "\n",
    "        lr = optimizer_G.param_groups[0]['lr']\n",
    "        print('\\nlearning rate = %.7f' % lr)\n",
    "\n",
    "        L1_total = 0\n",
    "        for batch_ids, batch in enumerate(train_loader): \n",
    "            \n",
    "            if (batch_ids+1) >= 2380 // opt.batch_size: break\n",
    "                \n",
    "            if (batch_ids+1) >= 100 // opt.batch_size: break\n",
    "            \n",
    "            real_A = [[], [], []]\n",
    "            real_A[0], real_A[1], real_A[2], real_B = preprocess_images(opt, batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                M0, _, _ = cloud_detection_model(real_A[0])\n",
    "                M1, _, _ = cloud_detection_model(real_A[1])\n",
    "                M2, _, _ = cloud_detection_model(real_A[2])\n",
    "            M = [M0, M1, M2]\n",
    "\n",
    "            real_A_combined = torch.cat(\n",
    "                (real_A[0], real_A[1], real_A[2]), 1).cuda()\n",
    "            real_A_input = torch.stack(\n",
    "                (real_A[0], real_A[1], real_A[2]), 1).cuda()\n",
    "\n",
    "            \"\"\"forward generator\"\"\"\n",
    "            fake_B, cloud_mask, aux_pred = model_GEN(real_A_input)\n",
    "\n",
    "            \"\"\"update Discriminator\"\"\"\n",
    "            set_requires_grad(model_DIS, True)\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            fake_AB = torch.cat((real_A_combined, fake_B), 1)\n",
    "            pred_fake = model_DIS(fake_AB.detach())\n",
    "            loss_D_fake = criterionGAN(pred_fake, False, noise)\n",
    "\n",
    "            real_AB = torch.cat((real_A_combined, real_B), 1)\n",
    "            pred_real = model_DIS(real_AB)\n",
    "            loss_D_real = criterionGAN(pred_real, True, noise)\n",
    "\n",
    "            loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            \"\"\"update generator\"\"\"\n",
    "            optimizer_G.zero_grad()\n",
    "            set_requires_grad(model_DIS, False)\n",
    "\n",
    "            fake_AB = torch.cat((real_A_combined, fake_B), 1)\n",
    "            pred_fake = model_DIS(fake_AB)\n",
    "            loss_G_GAN = criterionGAN(pred_fake, True, noise)\n",
    "\n",
    "            loss_G_L1 = criterionL1(fake_B, real_B) * opt.lambda_L1\n",
    "            L1_total += loss_G_L1.item()\n",
    "\n",
    "            loss_g_att = 0\n",
    "            for i in range(len(cloud_mask)):\n",
    "                loss_g_att += criterionMSE(cloud_mask[i]\n",
    "                                           [:, 0, :, :], M[i][:, 0, :, :])\n",
    "\n",
    "            if opt.aux_loss:\n",
    "                loss_G_aux = (criterionL1(aux_pred[0], real_B) + criterionL1(\n",
    "                    aux_pred[1], real_B) + criterionL1(aux_pred[2], real_B)) * opt.lambda_aux\n",
    "                loss_G = loss_G_GAN + loss_G_L1 + loss_g_att + loss_G_aux\n",
    "            else:\n",
    "                loss_G = loss_G_GAN + loss_G_L1 + loss_g_att\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            writer.add_scalar('training_G_GAN', loss_G_GAN, train_update)\n",
    "            writer.add_scalar('training_G_L1', loss_G_L1, train_update)\n",
    "            writer.add_scalar('training_D_real', loss_D_real, train_update)\n",
    "            writer.add_scalar('training_D_fake', loss_D_fake, train_update)\n",
    "            writer.add_scalar('training_D_fake', loss_g_att, train_update)\n",
    "            \n",
    "            \n",
    "            wandb.log({\n",
    "                'training_G_GAN': loss_G_GAN,\n",
    "                'training_G_L1': loss_G_L1,\n",
    "                'training_D_real': loss_D_real,\n",
    "                'training_D_fake': loss_D_fake,\n",
    "                'training_g_att': loss_g_att\n",
    "            }, step=train_update)\n",
    "            \n",
    "\n",
    "            pbar.update()\n",
    "            pbar.set_postfix(\n",
    "                G_GAN=f\"{loss_G_GAN:.4f}\",\n",
    "                G_L1=f\"{loss_G_L1:.4f}\",\n",
    "                G_L1_total=f\"{L1_total:.4f}\",\n",
    "                D_real=f\"{loss_D_real:.4f}\",\n",
    "                D_fake=f\"{loss_D_fake:.4f}\"\n",
    "            )\n",
    "            train_update += 1\n",
    "            \n",
    "        pbar.close()\n",
    "        \"\"\"validation\"\"\"\n",
    "        psnr, ssim = valid(opt, model_GEN, val_loader,\n",
    "                           criterionL1, writer, epoch)\n",
    "\n",
    "        if psnr_max < psnr:\n",
    "            psnr_max = psnr\n",
    "            torch.save(model_GEN.state_dict(), os.path.join(\n",
    "                opt.save_model_path, run_name, f'EP{epoch}_G_best_PSNR_{psnr:.3f}_SSIM_{ssim:.3f}.pth'))\n",
    "\n",
    "        if ssim_max < ssim:\n",
    "            ssim_max = ssim\n",
    "            torch.save(model_GEN.state_dict(), os.path.join(\n",
    "                opt.save_model_path, run_name, f'EP{epoch}_G_best_SSIM_{ssim:.3f}_PNSR_{psnr:.3f}.pth'))\n",
    "\n",
    "        scheduler_D.step()\n",
    "        scheduler_G.step()\n",
    "\n",
    "    print('Best PSNR: %.3f | Best SSIM: %.3f' % (psnr_max, ssim_max))\n",
    "\n",
    "def valid(opt, model_GEN, val_loader, criterionL1, writer, epoch):\n",
    "    model_GEN.eval()\n",
    "\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm.tqdm(total=len(val_loader), ncols=0,\n",
    "                     desc=\"Valid[%d/%d]\" % (epoch, opt.n_epochs), unit=\" step\")\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            \n",
    "            real_A = [[], [], []]\n",
    "            real_A[0], real_A[1], real_A[2], real_B = preprocess_images(opt, batch)\n",
    "\n",
    "            real_A_input = torch.stack(\n",
    "                (real_A[0], real_A[1], real_A[2]), 1).cuda()\n",
    "            fake_B, _, _ = model_GEN(real_A_input)\n",
    "\n",
    "            loss = criterionL1(fake_B, real_B)\n",
    "\n",
    "            for batch_idx in range(len(real_B)):\n",
    "                output, label = fake_B[batch_idx], real_B[batch_idx]\n",
    "                output_rgb, label_rgb = get_rgb(output), get_rgb(label)\n",
    "\n",
    "                psnr, ssim = psnr_ssim_cal(label_rgb, output_rgb)\n",
    "                psnr_list.append(psnr)\n",
    "                ssim_list.append(ssim)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.update()\n",
    "            pbar.set_postfix(\n",
    "                loss_val=f\"{total_loss:.4f}\"\n",
    "            )\n",
    "    psnr_list = np.array(psnr_list)\n",
    "    ssim_list = np.array(ssim_list)\n",
    "    psnr = np.mean(psnr_list)\n",
    "    ssim = np.mean(ssim_list)\n",
    "\n",
    "    writer.add_scalar('validation_PSNR', psnr, epoch)\n",
    "    writer.add_scalar('validation_SSIM', ssim, epoch)\n",
    "    pbar.set_postfix(loss_val=f\"{total_loss:.4f}\",\n",
    "                     psnr=f\"{psnr:.3f}\", ssim=f\"{ssim:.3f}\")\n",
    "\n",
    "    pbar.close()\n",
    "    \n",
    "    wandb.log({\"val_psnr\": psnr}, step=epoch)\n",
    "    wandb.log({\"val_ssim\": ssim}, step=epoch)\n",
    "    \n",
    "    return psnr, ssim\n",
    "\n",
    "train(opt, GEN, DIS, cloud_detection_model, optimizer_G,\n",
    "      optimizer_D, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "H3",
   "language": "python",
   "name": "h3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
